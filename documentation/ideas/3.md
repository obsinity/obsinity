# Histogram storage model

**What we store per bucketed time-window (e.g., 5s, 1m…):**

* **`bounds`**: the *fixed* upper bounds for the histogram, e.g.
  `[5, 10, 25, 50, 100, 250, 500, 1000, 2500, 5000, +Inf]` (ms)
  These are defined in the *metric derivation spec* and versioned. Every row for a given metric uses the same `bounds` version.
* **`counts[]`**: an integer array where `counts[i]` is the number of samples whose value falls in `(bound[i-1], bound[i]]` (with `bound[-1]` treated as `0`, and the last bucket as `+Inf`).
* **`sum` / `count`** (scalars): sum of sample values and total sample count, for averages/rates.

**Why arrays (dense) instead of maps (sparse):**

* Stable, fixed layout enables **fast pairwise aggregation**.
* Less overhead than JSON; trivial to roll up across time and nodes.
* You can still keep a JSON view if you need human-readable output.

---

# Rollup logic (time aggregation)

To roll up histograms **from 5s → 1m → 1h → 1d** (or across nodes):

1. **Precondition:** All children share the same `bounds` version.
2. **Element-wise sum of `counts[]`:**
   For each index `i`: `parent.counts[i] = Σ child.counts[i]`
3. **Add scalars:**
   `parent.sum = Σ child.sum`
   `parent.count = Σ child.count`

These operations are **associative and commutative**, so:

* Multiple workers can merge safely in any order.
* Late-arriving child buckets can be merged without reprocessing everything.

> PostgreSQL implementation note: pairwise summation can be done efficiently via **indexed array aggregation** (`generate_subscripts`) or a tiny helper function; some teams also use extensions that provide vectorized array ops. The important point is: **store counts as fixed-length arrays** so element-wise addition is trivial and safe.

---

# Merging across arbitrary time ranges

To compute histogram stats for “last 15 minutes”:

* Sum `counts[]` across all constituent windows (same bounds version).
* Sum `sum` and `count`.
* The result is the **aggregate histogram** for that range.

---

# Computing percentiles (p90 / p95 / p99)

Given:

* `bounds[]` (monotonic)
* `counts[]`
* `total = Σ counts[]`

Steps:

1. Choose your percentile, e.g. **p95** → `rank = 0.95 × total`.
2. Walk `counts[]` cumulatively until the running sum crosses `rank`.
   Suppose it crosses in bucket `k` with upper bound `U = bounds[k]` and lower bound `L = bounds[k-1]` (or 0 if `k=0`).
3. **Return a value within that bucket.** Options:

    * **Lower bound:** conservative (`L`).
    * **Upper bound:** optimistic (`U`).
    * **Midpoint:** `(L + U)/2` (common and simple).
    * **Linear interpolation inside the bucket** (better):
      Estimate the fraction within the bucket:

      ```
      in_bucket_before = cumulative - counts[k]
      within = (rank - in_bucket_before) / counts[k]
      p95 ≈ L + within × (U - L)
      ```
    * For the `+Inf` bucket, you can return `U` as `+Inf` or a configured **tail cap**.

Repeat the same process for **p90** / **p99** by changing the `rank`.

> Tip: For latency metrics, use **log-spaced bounds** (e.g., powers of \~√10) so bucket widths grow with value—this improves percentile accuracy in the tail while keeping bucket count modest (10–20 buckets usually suffice).

---

# Averages and rates from histograms

* **Mean** (over the aggregated range): `mean = sum / count`
* **Throughput**: `count / window_duration`
* **Error rate** (if you bucket error durations separately or tag samples): derive from the appropriate subset.

---

# Versioning & compatibility

* Keep `bounds_version` alongside each histogram row (and in the derivation spec).
* Only **merge** histograms with the same version.
  If you ever change bounds, treat it as a **new metric version** (or re-bucket old data offline if you truly must combine them).

---

# Concurrency & correctness

* Because the merge is **element-wise add**, you get:

    * **Idempotence** with a dedupe key (avoid double-adding the exact same child).
    * **Lock-free parallelism** via upserts that add counts (`counts[i] += …`) and sums.
* Late data is fine: it lands in the correct child window and gets merged at any time.

---

# Practical guardrails

* Keep bucket counts small (10–30) to minimize row width and merge cost.
* Maintain `sum`/`count` alongside `counts[]` to enable fast averages without reconstructing from arrays.
* For very high cardinality dimensions, consider:

    * Limiting which dims can produce histograms.
    * Or aggregating to coarser dims at higher time levels.

---

**TL;DR**

* Store histograms as **fixed bounds + counts\[] + sum + count**.
* **Rollups = element-wise add** for `counts[]`, plus scalar sums.
* Compute **p90/p95/p99** by scanning cumulative counts and interpolating within the located bucket.
* Use **versioned bounds**, **log-spaced buckets**, and keep operations **associative** so concurrency and late data are non-issues.
